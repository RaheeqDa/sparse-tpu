# Sparse TPU â€” Structured-Sparsity Tensor Accelerator

## ðŸ”Ž Overview
This project implements a parameterized **tensor processing accelerator** in SystemVerilog that exploits **2:4 structured sparsity**.  
The core architecture is based on a **systolic array** of processing elements (PEs), optimized for skipping zero-valued multiplications.  
The design philosophy: *only compute useful work, save power and time*.

---

## ðŸŽ¯ Goals
- Implement a **Multiply-Accumulate (MAC) PE** with sparsity-awareness.  
- Scale up to a **2D systolic array** (2Ã—2 â†’ 8Ã—8).  
- Support **INT8 Ã— INT8 â†’ INT32 accumulation**.  
- Integrate **simple memory buffers** (input/output scratchpads).  
- Add **control FSM** for loadâ€“computeâ€“drain sequencing.  
- Verify against a **Python golden model**.  

---

## ðŸ§® Why 2:4 Sparsity?
- Neural networks often contain redundant (â‰ˆ0) weights.  
- By pruning weights during training, frameworks like PyTorch/TensorFlow can enforce a **2 non-zeros per 4 values** pattern.  
- Hardware can then **skip 50% of multiplications** with minimal accuracy loss (<1%).  
- This method is inspired by NVIDIA Ampere GPUsâ€™ sparsity engines.  

---

## ðŸ“ Architecture (MVP v0.1)
- **PE Array:** Systolic, output-stationary dataflow.  
- **Sparsity Encoding:** 2-bit mask per group of 4 weights.  
- **Buffers:**  
  - A-buffer (activations, dense)  
  - B-buffer (weights + masks, sparse)  
  - C-buffer (results)  
- **Control FSM:**  
  - `IDLE â†’ LOAD_DATA â†’ COMPUTE â†’ DRAIN_RESULTS â†’ DONE`  

---

## âœ… Done Criteria (Milestone 1)
- Functional MAC PE verified against Python model.  
- 2Ã—2 systolic slice produces correct results on â‰¥100 randomized matrices.  
- Simulation shows â‰¥70% utilization on 50% sparse inputs.  
- Reset clears state in one cycle; handshake never deadlocks.  

---

## ðŸ“Š Out of Scope (v0)
- Full convolution layers  
- AXI/DDR memory controllers  
- Compiler/training integration  

---

## ðŸ› ï¸ Tools
- **Design:** SystemVerilog (Questa/ModelSim for simulation)  
- **Golden Model:** Python + NumPy  
- **Waveform Debug:** GTKWave or Questa GUI  
- **Version Control:** GitHub  

---

## ðŸ“… Roadmap
1. **Day 1â€“2**: Simple MAC + testbench  
2. **Day 3â€“4**: Add sparsity mask support  
3. **Week 1**: Build 2Ã—2 systolic slice  
4. **Week 2**: Memory buffers + FSM  
5. **Week 3â€“4**: Scale to 8Ã—8, performance analysis  
6. **Final**: Documentation + demo workloads  

---

## ðŸ“š References
- [Systolic Array Basics](https://www.google.com/search?q=systolic+array+matrix+multiplication)  
- [2:4 Structured Sparsity in NVIDIA Ampere](https://www.google.com/search?q=nvidia+ampere+2:4+sparsity)  
- [Roofline Model Performance Analysis](https://www.google.com/search?q=roofline+model+computing)  
